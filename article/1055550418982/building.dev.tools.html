<html>

<head>
<title>Building Development Tools</title>
</head>

<body bgcolor="#FFFFFF">

<table border="0" width="650" cellspacing="0" cellpadding="0" height="397">
  <tr>
    <td width="150" valign="top" height="235"><p align="center"><img
    src="http://www.antlr.org/logo.gif" alt="logo.gif (4249 bytes)" width="111" height="144"></p>
    <p align="center">&nbsp;</p>
    <p align="center">&nbsp;
    </td>
    <td width="20" valign="top"></td>
    <td valign="top">
<font face="ARIAL" size="-1">
<p>
<font size="+2">
Building Development Tools
</font>

<p>
<i> </i>

<p>

<p>With Greg Voss from JavaSoft.</p>

<p>Tue Feb 11, 1997</p>

<table border="0" cellpadding="3" width="100%">
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Hello everyone.<br>
        <br>
        Welcome to this Weeks JDC Discussion Forum,
        &quot;Building Java Development<br>
        Tools.&quot; We'll be talking today with Terence Parr,
        President and &quot;Lead<br>
        Mage&quot; (cofounder) of the MageLang Institute, which
        specializes in Java training.<br>
        <br>
        Hi Terence,<br>
        <br>
        By way of introduction I'll give some background on what
        led us to decide<br>
        to do a series of articles on parser technology and
        programmer development<br>
        tools.<br>
        <br>
        Parser technology is a subject near and dear to my heart.
        While<br>
        working at Borland, I was surprised to see how much of
        their work in UI<br>
        design was made possible by integrating parser technology
        into the<br>
        development environment. For instance Borland received
        many accolades<br>
        for it's syntax highlighting (keywords, method
        declarations, comments<br>
        appearing in alternate colors). It took some time for
        popular<br>
        programming editors such as Brief to follow suit. I
        realized that<br>
        third parties were at a distinct disadvatage, especially
        with languages<br>
        like C++, because they didn't have access to a parser.<br>
        <br>
        After seeing environments like Smalltalk, and NextStep
        with their wonderful<br>
        browsers and source code editors, I realized you could
        build all kinds of<br>
        amazing tools if you only had a parser. In fact many of
        the shell scripts<br>
        I was writing in grep, sed, awk and perl, were much
        better suited to small<br>
        programs based on having a separate parser.<br>
        <br>
        Can you talk a little bit about the kinds of tools you
        can build with parsers<br>
        especially for folks who don't think parsers having
        anything to do with<br>
        the kinds of problems they solve on a day to day basis? <br>
        <br>
        If anyone has any qestions, now would be a good time to
        submit them.<br>
        I'll continute with some of my own questions in the mean
        time.</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">As it turns out, the latest
        C/C++/Objective-C<br>
        browser under NeXTStep uses a PCCTS-generated<br>
        parser. Anyway, when you're a language guy,<br>
        all the World is a language recognition<br>
        or translation problem. For example, when I<br>
        did a stint at an Army supercomputing center,<br>
        even the physicists had to write parsers (in<br>
        Fortran...ack) to read in initial conditions<br>
        for their computational problems. Any program<br>
        that reads any kind of data or properties<br>
        can often take advantage of a &quot;real&quot; parser.<br>
        <br>
        Tools such as grep, sed, awk, and perl are<br>
        very nice and I use them a lot. However, they<br>
        are only useful on a line-by-line basis<br>
        generally. A parser can naturally span any<br>
        lexical boundaries you want.<br>
        <br>
        From a software engineering point of view,<br>
        anytime I can describe the structure of<br>
        my program in meta-language (such as a grammar),<br>
        I will do so. I view grammars as a<br>
        shorthand for what I could write manually.<br>
        <br>
        Note: I take all this grammar stuff pretty<br>
        far--I use grammars to describe tree data<br>
        structures so I don't have to build complicated<br>
        tree walks by hand.</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">But is this really relevant to me, Mr.
        Enterprise programmer, or an<br>
        ISV in a small shop. Isn't there a lot of overhead
        involved in building<br>
        a parser, or in learning how to use one?</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Greg, &quot;parsers aren't just for
        breakfast<br>
        anymore&quot;. ;)<br>
        <br>
        People often have a phobia concerning parsers<br>
        and translators because they are a pain to<br>
        build by hand. Further, the common LALR-based<br>
        parsing tools such as YACC require an MS degree<br>
        to really understand. The approach I have<br>
        taken is to help you build what you could<br>
        manually. The parsers generated from PCCTS<br>
        are recursive-descent C++, or Java programs<br>
        that you can debug and step through with your<br>
        favorite debugger. Believe me, there are<br>
        even biologists in UK right now doing DNA<br>
        sequence pattern matching with PCCTS. If<br>
        they can do it without a big background in<br>
        CS, CS folks should easily manage it.<br>
        <br>
        The production of parsers and translators<br>
        is aided by cutting-n-pasting rules or<br>
        whole grammars from public-domain sources,<br>
        or from grammars you've written before.<br>
        We are even adding &quot;grammar inheritance&quot; to<br>
        say that a new grammar is just like another<br>
        except for the following rules...<br>
        <br>
        As for applications of parsing in noncompiler development<br>
        shops, consider our CEO, Tom Burns, who starts to build<br>
        a large piece of software by first developing a language
        or<br>
        library to allow his group to speak in the problem
        domain.<br>
        (For example, a guy doing lots of set manipulations built
        a language<br>
        called SETL--set language--to allow him to more
        efficiently<br>
        describe his problems).<br>
        <br>
        Consider a shop that has lots of HTML documents to
        process and store, etc...<br>
        Think of all those stale links. Sure, there are tools to
        help you<br>
        manage this, but if you want to make true &quot;symbolic
        links&quot; to<br>
        documents, you must parse the HTML to grab these links.
        Also, think<br>
        of building your own indexer for a search engine. Walking
        all the<br>
        links in all documents requires an HTML parser. There are
        boatloads<br>
        of examples...<br>
        <br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">I was hoping you would bring up some of
        the programming tools, like<br>
        the source code migration tool we're building, to help
        identify (and<br>
        suggest or make changes) when moving JDK 1.0.2 source
        code to run<br>
        under JDK 1.1. There are many other examples (code
        mungers to help<br>
        protect your classes from being decompiled, class
        hierarchy browsers,<br>
        source code analysis tools and databases). How does
        having a parser<br>
        help you build a better source code migration tool than,
        say, an awk<br>
        script or a perl program?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">The answer lies in two key points:
        flexibility and recognition<br>
        strength (although ease of development also plays a big
        role).<br>
        <br>
        A &quot;real&quot; parser is much more flexible than
        something built from<br>
        a line-by-line tool such as awk or perl. For example, you
        can<br>
        have the parser build a intermediate representation such
        as<br>
        a parse tree or syntax tree that can be walked a second
        time.<br>
        Note that because Java allows you to reference a class or
        member<br>
        before seeing its definition, a two-pass parser is
        required. Doing<br>
        this in perl would require two different perl scripts
        with the second<br>
        script reading some output generated by the first plus,
        the original<br>
        input. Kinda gross. Further, if you want a dialog box to
        pop up<br>
        when the migration tool finds a potential trouble spot,
        you need<br>
        a real language...you can't have perl fork off a GUI very
        easily.<br>
        <br>
        The second key issue here relates to recognition
        strength. There<br>
        are some language constructs that simply require a great
        deal of<br>
        parser lookahead or context information to correctly
        parse. There<br>
        are even constructs that require symbol table information
        to parse<br>
        in a useful manner. A real parser has the infrastructure
        to solve<br>
        all of these issues, whereas perl was written as a report
        generator.<br>
        It is surely a force-fit to parse complicated languages
        with perl.<br>
        <br>
        In summary, you might say that perl is great at picking
        off a few<br>
        definitions here and there as long as you don't need
        context information.<br>
        It is not good at two-pass parsing and for building tree
        structures.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">I want to get back to symbol tables, and
        how to manipulate them,<br>
        later. For now, let's talk about basic technology issues
        first.<br>
        <br>
        Let's talk a little bit about the specifics or parser
        technology.<br>
        (Readers, keep in mind that you don't have to really
        understand how<br>
        parsers work or how to build them in order to use them in
        your own<br>
        custom development tools. However it does help to know
        what's<br>
        happening behind the scenes.)<br>
        <br>
        Can you briefly explain the difference between a
        &quot;parser&quot; and a &quot;parser generator.&quot;<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Okay, sure. A parser is a program that
        attempts to apply a grammatical<br>
        structure to an input stream of vocabulary symbols, such
        as programming<br>
        language keywords like &quot;int&quot; or
        &quot;while&quot;. Note I consider tree-walkers<br>
        parsers also--they simply apply a two-dimensional
        grammatical structure. ;)<br>
        Fundamentally, a parser answers &quot;yes&quot; or
        &quot;no&quot; as to whether the input<br>
        stream conformed to a grammatical structure.<br>
        <br>
        Now a parser-GENERATOR, is a tool that generates a parser
        from a high-level<br>
        description, such as a grammar. For example, the overall
        structure of<br>
        simple English language sentences might look like:<br>
        <br>
        sentence : subject predicate object ;<br>
        <br>
        A parser generator would read this description and
        generate a program to<br>
        check an input stream for conformance. You might get a
        method out like<br>
        this:<br>
        <br>
        void sentence() {<br>
        subject(); // go match a sentence subject such as
        &quot;I&quot;<br>
        predicate(); // match a predicate such as &quot;ran&quot;<br>
        object(); // match an object such as &quot;to the
        store&quot;<br>
        }<br>
        <br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">You mentioned that building languages
        applies to nonprogramming<br>
        languages--languages for controlling machine tools, or
        robots for<br>
        example. Have you had experience with special purpose
        langauges<br>
        such as those used in machine automation, or graphics
        generation?</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Actually, there are lots of fun things
        you can do with languages<br>
        as they apply to graphics. I built a really cool
        application that takes in<br>
        trees in LISP like notation such as &quot;(+ 3 4)&quot;
        and generates postscript<br>
        output to draw the tree visually (that tree has
        &quot;+&quot; at the root and two<br>
        children &quot;3&quot; and &quot;4&quot;). All the trees
        appearing in the PCCTS book were<br>
        generated automatically using this tool.<br>
        <br>
        Another groovy language you might be interested in is
        called IVL: Inventor<br>
        Language (VRML is basically the ASCII output of the SGI
        Inventor library).<br>
        IVL has English commands to build up complicated VRML
        scenes. For example:<br>
        <br>
        floor = &quot;http://some URL of the floor VRML
        object&quot;<br>
        table = ...<br>
        ...<br>
        draw floor<br>
        draw table above floor<br>
        draw workstation above table<br>
        draw refrigerator above floor and 3 to the left of table<br>
        <br>
        See http://www.ocnus.com/ivl.html for more info. Oh, IVL
        is pronounced &quot;evil&quot; ;)<br>
        <br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Let's talk about ANTLR, your parser
        generator. First of all, what<br>
        does ANTLR stand for?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">ANTLR stands for ANother Tool for
        Language Recognition. A &quot;funny&quot; term as there
        was YACC, then BISON, then ANTLR. ;)<br>
        ANTLR is the parser generator packaged in PCCTS (Purdue
        Compiler-Construction Tool Set)--the practical
        application of<br>
        my PhD dissertation at Purdue.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">ANTLR takes a Java grammar (essentially
        compatible with JDK 1.1 Java<br>
        code--except for inner classes--and generates C++ source
        code for a<br>
        parser. <br>
        <br>
        <br>
        I guess we can get back to symbol tables from here. The
        output of<br>
        ANTLR can either be a symbol table (a database full of
        identifiers<br>
        such as method and variables, paired with their
        definitions), or it<br>
        could be just a bunch of print statements, for example a
        list of<br>
        filenames and line numbers followed by lines with the
        matched<br>
        indentifiers.<br>
        <br>
        When would you want symbol table output versus
        printf-style<br>
        text output?</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Just to clarify a bit...ANTLR doesn't
        generate a symbol table, it generates a parser that can
        construct a symbol table for later use by your program.
        Another thing to consider is that the parser can also
        generate an intermediate representation in the form of a
        tree data structure that can be easily walked to obtain
        information or manipulated for eventual translation back
        to text.<br>
        <br>
        Anyway, the task at hand really defines what you want to
        output. Can you be more specific about printf versus
        symbol table stuff?</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">What I'm really after here is an
        understanding of how much a<br>
        programmer would need to customize the generated parser
        source code to<br>
        get output that is going to be useful for the custom tool
        she or he<br>
        needs to build.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Ah. Well, you never touch the output of
        the parser; rather, you embed Java or C++ actions in the
        grammar, which are then executed at that point during the
        parse. For example, here is the rule for class
        definitions in our Java grammar:<br>
        <br>
        classDefinition<br>
        : &quot;class&quot; id:IDENT extends implements<br>
        &lt;<<BR> currentClassOrInterface = id-&amp;gtgetText();<br>
        fprintf(out,&quot;class %s %d\n&quot;,
        id-&amp;gtgetText(), id-&amp;gtgetLine());<br>
        &gt;&gt;<br>
        classBlock<br>
        ;<br>
        <br>
        The printf will dump out the class def right before the
        classBlock is matched.<br>
        <br>
        This type of output is easy to obtain, but is often not
        what you want. For difficult problems such as
        translations that require order of output differences you
        cannot just use a bunch of printfs. You must build a
        symbol table and generate a tree (perhaps ordering the
        list of declarations such that no member is referenced
        before it is used). The tree is then manipulated and
        eventually printed back to text.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top"><br>
        What does a programmer gain by having a parser generator
        that generates<br>
        C/C++ code as opposed to a parser generator that
        generates Java code?<br>
        <br>
        Is it a disadvantage, for example, that ANTLR generates
        C/C++ parser<br>
        source code rather than Java source code?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">There are two main advantages to using
        C++ as the implementation vehicle for a parser. First,
        C++ is currently faster than Java and, second, most
        development tools right now are written in C++ allowing
        these parsers to be easily folded into their
        environments. Note, however, that Java can still run
        these parsers using Runtime.exec(). We have demonstrated
        this notion by connecting a Java-based source code
        browser and connecting to a Java parser written in C++
        (future article at this site).<br>
        <br>
        At the moment were are using the current 1.33 version of
        PCCTS that generates parsers implemented in C and C++
        only. ANTLR 2.00, under development, is written in Java
        and generates Java. A prerelease version will be
        available shortly. Join our ANTLR 2.00 discussion group
        if you want to influence its design (send a body of
        &quot;subscribe&quot; to
        antlr-interest-request@java.magelang.com, then post by
        emailing to antlr-interest@java.magelang.com).<br>
        <br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Where can programmers get ANTLR, PCCTS,
        and your new rewrite of ANTLR that generates a Java
        parser?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Programmers should take a look at
        http://java.magelang.com/antlr/entry.html<br>
        to get started.<br>
        <br>
        Watch comp.compilers.tools.pccts or the antlr-interest
        list for an announcement<br>
        concerning the latest PCCTS.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Also, readers should know about the
        series of articles written for<br>
        the JDC Web site. I think the third installment is going
        out this<br>
        week. There's an interesting example there of a source
        code browser--<br>
        at least as a proof of concept example.<br>
        Can you explain terms about grammars like LALR, LL(k),
        LL(1)? I'm<br>
        sure these terms must really seem intimidating if you
        haven't studied<br>
        parser technology in a formal setting.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">You're right. These terms are confusing
        and often difficult to explain succinctly and correctly.<br>
        People often say, &quot;okay, I don't care what LALR or
        LL parsers are, but YACC is LALR and PCCTS<br>
        is LL. What is the *difference* between the two in
        practical terms?&quot; There are three primary
        differences: recognition strength, semantic flexbility
        (you can embed actions anywhere you want in the grammar),
        and complexity (LALR is a lot harder to understand
        because it requires actual parsing theory knowledge).<br>
        <br>
        Recognition strength is considered by many to be the
        important issue.<br>
        After thinking about parsers for a long time, I've
        settled on the idea that strength is all about context
        and lookahead. In other words, the more information
        available to the parser to make decisions, the more
        complicated the language it can recognize.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Can you say a bit more about LALR and
        LL(k)--what are the advantages<br>
        of one over the other, how do they work? FYI, Terence is
        composing a (lengthy answer) Give him a beer and ask<br>
        him to talk about LALR, and you're set for the evening.
        This will be<br>
        the last answer posted today.</td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Well, I'll start by bludgeoning you with
        LALR-based parsers so that you<br>
        get a nice feeling when you think about LL ;)<br>
        <br>
        LALR parsers are a simplified version of LR parsers
        (L==parse input from left to right, R==make decisions at
        the right edge). LALR's recognition strength relies upon
        large amounts of context. The context of an LR or LALR
        parser consists of all grammar alternatives consistent
        with the previously seen input. This context often
        includes several &quot;pending&quot; alternatives.
        Intuitively, an LR parser attempts to match multiple
        productions at the same time and postpones making a
        decision until sufficient input has been seen. <br>
        <br>
        Consider a simple grammar in order for me to illustrate
        what that means.<br>
        <br>
        rule : A B C<br>
        | A B D<br>
        ;<br>
        <br>
        The capital letters are vocabulary symbols (token
        references). An LALR parser consumes input symbols until
        it finds a (viable) complete production; here, we can
        assume all productions are viable. The parser reads input
        symbols until it has &quot;A B C&quot;, for example, at
        which point it decides that it has matched the first
        alternative of the rule. Similarly, for input &quot;A B
        D&quot;, the parser would not decide between alternatives
        one and two until it had seen the &quot;D&quot;. The fact
        that the parser delays making decisions until right edges
        , provide the source of the R in LR and LALR.<br>
        <br>
        In contrast, the context for an LL parser is restricted
        to the sequence of previously matched productions, and
        the position within the current grammar production being
        matched. An LL parser must make decisions about which
        production to match without having seen any portion of
        the pending productions--it has access to less context
        information. You might say that an LALR parser asks
        &quot;What alternative does the input match?&quot; and an
        LL parser says &quot;I'm looking to match rule foo. Is
        the input consistent with that rule?&quot; For our simple
        grammar, you would say that an LL parser needs to decide
        between both alternatives without having matched any of
        the alternative. The parser needs to see three symbols
        ahead (past the &quot;A B&quot; to the &quot;D&quot; or
        &quot;C&quot;) in order to make a correct decision--the
        second L in LL comes from the fact that decisions are
        made at the left-edge of alternatives.<br>
        <br>
        Due to the fact that LL parsers have access to less
        context information, LL parsers rely heavily on
        lookahead. Note that our grammar above is LALR(0) (look
        ahead 0 tokens) and LL(3) (look ahead 3 tokens) as a case
        in point. Parser lookahead is like going into a dark maze
        that has words written on the floor at regular
        intervals--given the right word sequence, an adventurer
        can successfully traverse the maze. At each fork in the
        maze, the adventurer must compare the input (the visible
        words on the floor) with the grammar (key word sequence
        allowing escape). What happens if at a fork both paths
        start with the same word on the floor? You have a
        nondeterminism. More lookhead (or backtracking) is
        required to make a correct decision. Naturally, the
        answer is to get a bigger flashlight. ;)<br>
        <br>
        Tough to get a correct and readable answer on the fly,
        but...<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Wow! That's some answer Terence. I like
        the analogy that you <br>
        use at the end about going through a maze with words
        written on<br>
        the floor. Makes a token stream more graphic. Maybe we
        could write<br>
        a game program to get kids messing with parsers like they
        do by<br>
        combining blasteroids with spelling programs :)<br>
        We've gone far beyond our allotted time. Hope no one
        minded too<br>
        much. :) Thanks, Terence for giving us some insight into
        parsing and<br>
        its use in building custom programming tools. I'm looking
        forward to<br>
        seeing the Java version of ANTLR. We'll post it to the
        JDC web site<br>
        when it's ready. We'll also be posting some pretty cool
        tools that<br>
        use the parser to do their magic.<br>
        <br>
        Anything you want to add, Terence?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>parrty:</NOBR></td>
        <td valign="top">Nope. Glad to get a chance to talk about
        language tools and such. I look forward to hearing from<br>
        anybody with further questions on development tools
        (parrt@MageLang.com).<br>
        <br>
        -- parrt signing off<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Thanks everyone for attending. See you
        next week.<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>echarne:</NOBR></td>
        <td valign="top">Sorry I'm way late, but is there a
        publically available parser available for<br>
        Java?<br>
        </td>
    </tr>
    <tr>
        <td align="right" valign="top"><NOBR>gvoss:</NOBR></td>
        <td valign="top">Yes, there is. Keep following the JDC
        article series. There's one available<br>
        for download now (ANTLR) through JDC or through the URL
        Terence<br>
        mentioned earlier. There is both a parser and and parser
        generator<br>
        available. The current version of the parser is in C++ so
        it is hard to<br>
        use with an applet. But a regular Java program can call
        it as a stand-alone<br>
        executable.<br>
        <br>
        Shortly, we'll post a new version of ANTLR that genarates
        a parser written<br>
        in Java (that also recognizes Java source). This would
        work in applets<br>
        as well as programs. It also has the advantage of being
        easier to integrate<br>
        with your application--be it a Java source code editor,
        class browser,<br>
        or whatever. Hope that helps. So long, everyone, time for
        lunch here on the West Coast. Last moderator (me) signing
        off. The forum is now unmoderated</td>
    </tr>
</table>
</font>
</td>
  </tr>
</table>
</body>
</html>
